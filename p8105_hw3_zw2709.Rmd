---
title: "p8105_hw3_zw2709"
output: github_document
---

```{r setup, include = FALSE}
library(tidyverse)
knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = 0.6,
  out.width = '90%'
)
theme_set(theme_minimal()+ theme(legend.position = 'bottom'))

options(
  ggplot2.continuous.colour = 'viridis',
  ggplot2.continuous.fill = 'viridis'
)

scale_colour_discrete = scale_color_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```


## Problem 1

```{r}
library(p8105.datasets)
data("instacart")
```
This dataset contains `r nrow(instacart)` rows and `r ncol(instacart)` columns.

Observations are the level of items in orders by user. There are user / order variables -- user ID, order ID, order day, and order hour. There are also item variables -- name, aisle, department, and some numeric codes. 

How many aisles, and which are most items from?

```{r}
instacart %>% 
	count(aisle) %>% 
	arrange(desc(n))
```


Let's make a plot

```{r}
instacart %>% 
	count(aisle) %>% 
	filter(n > 10000) %>% 
	mutate(
		aisle = factor(aisle),
		aisle = fct_reorder(aisle, n)
	) %>% 
	ggplot(aes(x = aisle, y = n)) + 
	geom_point() + 
	theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))
```


Let's make a table!!

```{r}
instacart %>% 
	filter(aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>% 
	group_by(aisle) %>% 
	count(product_name) %>% 
	mutate(rank = min_rank(desc(n))) %>% 
	filter(rank < 4) %>% 
	arrange(aisle, rank) %>% 
	knitr::kable()
```


Apples vs ice cream..

```{r}
instacart %>% 
	filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>% 
	group_by(product_name, order_dow) %>% 
	summarize(mean_hour = mean(order_hour_of_day)) %>% 
	pivot_wider(
		names_from = order_dow,
		values_from = mean_hour
	)
```





## Problem 2


### 1)

First import the data.
Then load, tidy and wrangle data.

```{r}
data =
  read_csv('./data/accel_data.csv') %>% 
  janitor::clean_names() %>% 
  
  pivot_longer(
    activity_1 : activity_1440,
    names_to = 'activity_minute',
    names_prefix = 'activity_',
    values_to = 'activity_counts'
  ) 
  

```
Increase a new variable.

```{r}

data_ = mutate(
  data,
  vs = case_when(
    day %in% c('Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday') ~'weekday',
    day %in% c('Saturday', 'Sunday') ~'weekend',
    TRUE ~ ''
  ) 
)


```



Then we encode the data and show them in order


We need to change the variable classes.

```{r}
data_classes = 
  data_ %>% 
  mutate(
    activity_minute = as.numeric(activity_minute),
    activity_counts = as.numeric(activity_counts),
    week = as.numeric(week),
    day_id = as.numeric(day_id),
    day = as.character(day),
    vs = as.character(vs)
    
  )
```

Have another 'day_df' variable whose class is integer to denote the 'day' character, so that the order based on week and day would be satisfied.
```{r}

day_df = 
  tibble(
    day_number = 1:7,
    day = c('Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday')
  )

data_new = 
  left_join(data_classes, day_df, by = 'day') %>% 
  relocate(week, day, vs) %>% 
  arrange(week, day_number) %>% 
  select(-day_number) 



```

As is shown in 'data_new', there are six variables in total:`r names(data_new)`

The variable 'day','vs','activity_name' are characters and the remaining are numerical measures.

The dataset has `r nrow(data_new)` rows, and has `r nrow(data_new)` observations.





### 2)


Create a table

```{r}

data_new %>% 
  group_by(week, day) %>% 
  summarize(sum_activity_counts = sum(activity_counts)) %>% 
  pivot_wider(
    names_from = week, 
    values_from = sum_activity_counts
  ) %>% 
  knitr::kable()
```





The table reflects data about 7 days of one week and the total 5 weeks, and the data were the total number of the activity counts.

For the first two weeks, the most active activity counts are during weekends(Sunday for week 1, Saturday for week 2). For the remaing three weeks, the most active activities counts occur during the weekdays.

For week 4 and week 5, the activity level for Saturday seems to be low compared with others.





### 3)

The single-panel plot would have x axis of the 24 hours with the measurement of time, and would have y axis of the activity counts. Then we use different colors of lines to represent different days of the week.

```{r}
data_plot = 
  data_new %>% 
  ggplot(aes(x = activity_minute, y = activity_counts, color = day)) +
  geom_point(alpha = 0.5) +
  labs(
    title = "Activity level",
    x = "time measurement",
    y = "activity counts"
  ) + 
  
  scale_x_continuous(
    breaks = c(0,60,120,180,240,300,360,420,480,540,600,660,720,780,840,900,960,1020,1080,1140,1200,1260,1320,1380,1440),
    labels = c("0","1h","2h","3h","4h","5h","6h","7h","8h","9h","10h","11h","12h","13h","14h","15h","16h","17h","18h","19h","20h","21h","22h","23h","24h")
  ) +

  scale_y_continuous(
  
    trans = "sqrt", 
    position = "right"
  ) + 
  geom_line() +
  
  viridis::scale_color_viridis(
    name = 'day of week',
    discrete = TRUE) +
  theme_minimal()+
  theme(legend.position = 'bottom')
```



```{r fig.width=12, fig.height=6}
data_plot
```



From the perspective of 24 hours in a day, the midnight time would have low level of activity level (midnight to about 6 o'clock). The time period between 20 p.m. to 22 p.m. at night seem to have higher values of activity counts and are regarded to be active.

From the weekdays, it seems that the peak activity time of 20 p.m. to 22 p.m. occurs frequently for Friday, meaning the relative active status of Friday night.








## Problem 3



import data
```{r}
library(p8105.datasets)
data("ny_noaa")
```

The dataset contains information about core variables for all New York state weather stations during some time period. 



The variables contained are `r names(ny_noaa)`.
So the key variables besides 'id' and 'date' are: prcp, snow, snwd, tmax, tmin, which are some key weather variables of the stations.



These five variables all contain missing values, and the number of missing values are as follows:
prcp: `r sum(is.na(ny_noaa$prcp))`; snow: `r sum(is.na(ny_noaa$snow))`; snwd: `r sum(is.na(ny_noaa$snwd))`; tmax: `r sum(is.na(ny_noaa$tmax))`; tmin: `r sum(is.na(ny_noaa$tmin))`. 

Overall, the 'tmax' and 'tmin' contain a relatively high level of missing values, might reduce the accuracy of result to some extent.





The dataset has `r nrow(ny_noaa)` rows and `r ncol(ny_noaa)` columns, so the size of dataset is `r nrow(ny_noaa)` * `r ncol(ny_noaa)`.






### 1)

data cleaning

```{r}
ny_data = 
  ny_noaa %>% 
  drop_na() %>% 
  mutate(date, date = as.character(date)) %>% 
  separate(date, into = c('year', 'month', 'day')) %>% 
  
  mutate(prcp, prcp = as.numeric(prcp)) %>% 
  mutate(snow, snow = as.numeric(snow)) %>% 
  mutate(snwd, snwd = as.numeric(snwd)) %>% 
  mutate(tmax, tmax = as.numeric(tmax)) %>% 
  mutate(tmin, tmin = as.numeric(tmin))
```









Since 'prcp' measures Precipitation (tenths of mm), 'tmax' and 'tmin' measures temperatures in tenths of degrees C, so we divide each value by ten to get reasonable units.

```{r}
ny_data_ = 
  ny_data %>% 
  mutate(
    tmax = tmax / 10,
    tmin = tmin / 10, 
    prcp = prcp / 10
  )
```


For snowfall, the most commonly observed values are 0 with the number of 111758, meaning the station did not have snow during specific period.
```{r}
ny_data_ %>% 
  count(snow) %>% 
	arrange(desc(n)) 
```






### 2)

Make the two-panel plot.


The plot may consist of two sub-plots for January and July. For each sub-plot, the x-axis is measurement of years denoted by 'year', and the y-axis is the average max temperatures denoted by 'tmax', and the color of the lines represent different stations.



```{r}
ny_data_ %>% 
  filter(month %in% c('01','07')) %>% 
  group_by(year, id, month, tmax) %>% 
  summarize(
    mean_tmax = mean(tmax, na.rm = TRUE)
  ) %>% 

  ggplot(aes(x = year, y = mean_tmax, color = id))+
  geom_point(alpha = 0.5)+
  geom_smooth(se = FALSE) +
  labs(
    title = "temperature level",
    x = "year",
    y = "average max temperature"
  ) + 

  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))+
  theme(legend.position = 'none') +
  facet_grid(month ~ .)

```



It is observed that the average max temperatures in July (around 20-40 degrees)is higher than that in January (around -20-20 degrees). 


The range of temperatures (max-min) in January seems to be larger than that of July, meaning more changes in max temperature and unstable status.


There are some outliers both for the two months. The outliers in January seem to be seen obviously seen at around 1995 for over 20 degrees.


The outliers in July are usually lower than the average level, and some obvious ouliers occur at about 1989,1991 and 2000.






### 3)

Then make the two-panel plot.


For the first plot, it is tmax vs tmin for the full dataset.



For the second plot showing the distribution of snowfall values greater than 0 and less than 100 separately by year.



Then we need to combine two sub-plots to get the two-panel plot.



```{r}
library(patchwork)

temp = 
  ny_data_ %>% 
  ggplot(aes(x = tmin, y = tmax)) +
  geom_hex()+
  labs(
    title = "Temperature plot",
    x = "Minimum temperature (C)",
    y = "Maxiumum temperature (C)"
  ) +
  viridis::scale_color_viridis(
    name = 'Location',
    discrete = TRUE)




snowfall = 
  ny_data_ %>% 
  filter(snow > 0 & snow < 100) %>% 
  ggplot(aes(x = year, y = snow)) +   #x为name，显示三个图
  geom_boxplot()+
  labs(
    title = "snowfall",
    x = "year",
    y = "snowfall"
  ) +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))
  
temp_snowfall_plot = temp / snowfall

```


```{r fig.width=8, fig.height=10}
temp_snowfall_plot
```












